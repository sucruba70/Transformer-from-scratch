{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f415ad4f",
   "metadata": {},
   "source": [
    "## LayerNorm\n",
    "\n",
    "1. **평균($\\mu$)과 분산($\\sigma^2$) 구하기**\n",
    "   - 입력 벡터($x$)의 차원($d$) 내에서 통계량을 계산\n",
    "   $$\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i$$\n",
    "   $$\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2$$\n",
    "\n",
    "2. **정규화 (Normalization)**\n",
    "   - 평균이 0, 분산이 1이 되도록 값을 조정 ($\\epsilon$은 분모가 0이 되는 것을 막기 위한 작은 수)\n",
    "   $$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "3. **스케일링 & 이동 (Scale & Shift)**\n",
    "   - 단순히 정규화만 하면 데이터가 가진 고유한 표현력이 사라질 수 있음\n",
    "   - 학습 가능한 파라미터인 감마($\\gamma$)와 베타($\\beta$)를 도입해서, 모델이 알아서 적절한 범위로 조절\n",
    "   $$y_i = \\gamma \\cdot \\hat{x}_i + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b9d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693cdd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.,  10., 100.])\n",
      "tensor(37.)\n",
      "tensor(1998.)\n",
      "tensor(44.6990)\n",
      "tensor([-0.8054, -0.6040,  1.4094])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 10.0, 100.0])\n",
    "print(x)\n",
    "\n",
    "mean = x.mean()\n",
    "var = x.var(unbiased=False) # 모분산\n",
    "std = torch.sqrt(var + 1e-6)\n",
    "\n",
    "print(mean)\n",
    "print(var)\n",
    "print(std)\n",
    "\n",
    "x_norm = (x - mean) / std\n",
    "print(x_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7b08bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# L, dim\n",
    "L, dim = 10, 256\n",
    "x = torch.randn(L, dim)\n",
    "print(x.shape)\n",
    "\n",
    "mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "var = torch.var(x, dim=-1, unbiased=False ,keepdim=True)\n",
    "std = torch.sqrt(var + 1e-6)\n",
    "\n",
    "x_norm = (x-mean) / std\n",
    "print(x_norm.shape)\n",
    "\n",
    "gamma = torch.ones(dim)\n",
    "beta = torch.zeros(dim)\n",
    "\n",
    "y = x_norm * gamma + beta\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210958cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 512])\n",
      "torch.Size([4, 10, 512])\n",
      "torch.Size([4, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization\n",
    "# B, L, dim\n",
    "B, L, dim = 4, 10, 512\n",
    "x = torch.randn(B, L, dim)\n",
    "print(x.shape)\n",
    "\n",
    "mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "var = torch.var(x, dim=-1, unbiased=False ,keepdim=True)\n",
    "std = torch.sqrt(var + 1e-6)\n",
    "\n",
    "x_norm = (x-mean) / std\n",
    "print(x_norm.shape)\n",
    "\n",
    "gamma = torch.ones(dim)\n",
    "beta = torch.zeros(dim)\n",
    "\n",
    "y = x_norm * gamma + beta\n",
    "\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "126415d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(\n",
    "            self,\n",
    "            normalized_shape: int,\n",
    "            eps: float = 1e-5,\n",
    "            elementwise_affine: bool = True,\n",
    "            bias: bool = True,\n",
    "    ):\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.training = True\n",
    "\n",
    "        if elementwise_affine:\n",
    "            self.gamma = torch.ones(self.normalized_shape, requires_grad=True)\n",
    "            self.beta = None\n",
    "            if bias:\n",
    "                self.beta = torch.zeros(self.normalized_shape, requires_grad=True)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "            self.beta = None    \n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.shape[-1] == self.normalized_shape\n",
    "        # 실제 입력 [B, L, dim]\n",
    "        # [B, L, 1]\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True) \n",
    "        # [B, L, 1]\n",
    "        var = torch.var(x, dim=-1, unbiased=False, keepdim=True)\n",
    "        #var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
    "        # [B, L, 1]\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "\n",
    "        # [B, L, dim]\n",
    "        x_norm = (x - mean) / std\n",
    "\n",
    "        if self.gamma is None:\n",
    "            y = x_norm\n",
    "        elif self.beta is None:\n",
    "            y = self.gamma * x_norm\n",
    "        else:\n",
    "            y = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def parameters(self):\n",
    "        if self.gamma is None:\n",
    "            return []\n",
    "        elif self.beta is None:\n",
    "            return [self.gamma]\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        self.training = mode\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "B, L, D = 4, 10, 512\n",
    "x = torch.randn(B, L, D)\n",
    "\n",
    "ln = LayerNorm(D, eps=1e-5, elementwise_affine=False, bias=False)\n",
    "y = ln(x)\n",
    "\n",
    "assert y.shape == x.shape, \"Error\"\n",
    "\n",
    "mean = y.mean(dim=-1)\n",
    "var = y.var(dim=-1, unbiased=False)\n",
    "\n",
    "assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-4, rtol=0), \"Mean not ~0.\"\n",
    "assert torch.allclose(var, torch.ones_like(var), atol=1e-3, rtol=0), \"Var not ~1.\"\n",
    "\n",
    "x = torch.randn(B, L, D)\n",
    "\n",
    "ln = LayerNorm(D, eps=1e-5, elementwise_affine=True, bias=True)\n",
    "test = nn.LayerNorm(D, eps=1e-5, elementwise_affine=True, bias=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test.weight.copy_(ln.gamma)\n",
    "    test.bias.copy_(ln.beta)\n",
    "\n",
    "y = ln(x)\n",
    "y_test = test(x)\n",
    "\n",
    "max_abs_err = (y - y_test).abs().max().item()\n",
    "\n",
    "assert torch.allclose(y, y_test, atol=1e-6, rtol=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdc0b3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] gradient flow test (gamma/beta grads exist & finite)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "B, L, D = 4, 5, 16\n",
    "x = torch.randn(B, L, D, requires_grad=True)\n",
    "\n",
    "ln = LayerNorm(D, elementwise_affine=True, bias=True)\n",
    "\n",
    "y = ln(x)\n",
    "loss = (y ** 2).mean()\n",
    "loss.backward()\n",
    "\n",
    "grads = [p.grad for p in ln.parameters()]\n",
    "assert all(g is not None for g in grads), \"Some parameters have no grad.\"\n",
    "assert all(torch.isfinite(g).all() for g in grads), \"Some grads have NaN/Inf.\"\n",
    "\n",
    "print(\"[OK] gradient flow test (gamma/beta grads exist & finite)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506f298",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "1. 평균($\\mu_{\\mathcal{B}}$)과 분산($\\sigma_{\\mathcal{B}}^2$) 구하기\n",
    "    - 배치($m$) 전체를 보고, 같은 위치(Feature)에 있는 값들의 통계량을 계산\n",
    "    - $m$: 배치 크기 (Batch Size)\n",
    "    $$\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i$$\n",
    "    $$\\sigma_{\\mathcal{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\mathcal{B}})^2$$\n",
    "1. 정규화 (Normalization)\n",
    "    - 배치 내에서 평균이 0, 분산이 1이 되도록 값을 조정 ($\\epsilon$은 안정성을 위한 작은 수)\n",
    "    $$\\hat{x}_i = \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}}\n",
    "1. $$스케일링 & 이동 (Scale & Shift)\n",
    "    - 정규화된 값이 데이터의 고유한 특징을 잃지 않도록 조정\n",
    "    - 학습 가능한 파라미터 감마($\\gamma$)와 베타($\\beta$) 사용 (채널/특성별로 존재)\n",
    "    $$y_i = \\gamma \\cdot \\hat{x}_i + \\beta$$\n",
    "\n",
    "Note: 학습(Train) 때는 위 계산을 매번 수행하지만, 추론(Inference/Eval) 때는 학습 중 구해놓은 '이동 평균(Moving Average)'을 사용한다는 점이 LayerNorm과 가장 큰 차이입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08107c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 512])\n",
      "torch.Size([4, 10, 512])\n",
      "torch.Size([4, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Batch Normalization\n",
    "# B, L, dim\n",
    "B, L, dim = 4, 10, 512\n",
    "x = torch.randn(B, L, dim)\n",
    "print(x.shape)\n",
    "\n",
    "mean_bn = torch.mean(x, dim=0, keepdim=True)\n",
    "var_bn = torch.var(x, dim=0, unbiased=False ,keepdim=True)\n",
    "std_bn = torch.sqrt(var_bn + 1e-6)\n",
    "\n",
    "x_norm = (x-mean_bn) / std_bn\n",
    "print(x_norm.shape)\n",
    "\n",
    "gamma = torch.ones(dim)\n",
    "beta = torch.zeros(dim)\n",
    "\n",
    "y = x_norm * gamma + beta\n",
    "\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa1dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
