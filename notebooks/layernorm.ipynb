{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f415ad4f",
   "metadata": {},
   "source": [
    "## LayerNorm (층 정규화)\n",
    "\n",
    "1. **평균($\\mu$)과 분산($\\sigma^2$) 구하기**\n",
    "   - 입력 벡터($x$)의 차원($d$) 내에서 통계량을 계산\n",
    "   $$\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i$$\n",
    "   $$\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2$$\n",
    "\n",
    "2. **정규화 (Normalization)**\n",
    "   - 평균이 0, 분산이 1이 되도록 값을 조정 ($\\epsilon$은 분모가 0이 되는 것을 막기 위한 작은 수)\n",
    "   $$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "3. **스케일링 & 이동 (Scale & Shift)**\n",
    "   - 단순히 정규화만 하면 데이터가 가진 고유한 표현력이 사라질 수 있음\n",
    "   - 학습 가능한 파라미터인 감마($\\gamma$)와 베타($\\beta$)를 도입해서, 모델이 알아서 적절한 범위로 조절\n",
    "   $$y_i = \\gamma \\cdot \\hat{x}_i + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693cdd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.,  10., 100.])\n",
      "tensor(37.)\n",
      "tensor(1998.)\n",
      "tensor(44.6990)\n",
      "tensor([-0.8054, -0.6040,  1.4094])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 10.0, 100.0])\n",
    "print(x)\n",
    "\n",
    "mean = x.mean()\n",
    "var = x.var(unbiased=False) # 모분산\n",
    "std = torch.sqrt(var + 1e-6)\n",
    "\n",
    "print(mean)\n",
    "print(var)\n",
    "print(std)\n",
    "\n",
    "x_norm = (x - mean) / std\n",
    "print(x_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a7b08bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# L, dim\n",
    "L, dim = 10, 256\n",
    "x = torch.randn(L, dim)\n",
    "print(x.shape)\n",
    "\n",
    "mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "var = torch.var(x, dim=-1, unbiased=False ,keepdim=True)\n",
    "std = torch.sqrt(var + 1e-6)\n",
    "\n",
    "x_norm = (x-mean) / std\n",
    "print(x_norm.shape)\n",
    "\n",
    "gamma = torch.ones(dim)\n",
    "beta = torch.zeros(dim)\n",
    "\n",
    "y = x_norm * gamma + beta\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210958cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 512])\n",
      "torch.Size([4, 10, 512])\n",
      "torch.Size([4, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization\n",
    "# B, L, dim\n",
    "B, L, dim = 4, 10, 512\n",
    "x = torch.randn(B, L, dim)\n",
    "print(x.shape)\n",
    "\n",
    "mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "var = torch.var(x, dim=-1, unbiased=False ,keepdim=True)\n",
    "std = torch.sqrt(var + 1e-6)\n",
    "\n",
    "x_norm = (x-mean) / std\n",
    "print(x_norm.shape)\n",
    "\n",
    "gamma = torch.ones(dim)\n",
    "beta = torch.zeros(dim)\n",
    "\n",
    "y = x_norm * gamma + beta\n",
    "\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506f298",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "1. 평균($\\mu_{\\mathcal{B}}$)과 분산($\\sigma_{\\mathcal{B}}^2$) 구하기\n",
    "    - 배치($m$) 전체를 보고, 같은 위치(Feature)에 있는 값들의 통계량을 계산\n",
    "    - $m$: 배치 크기 (Batch Size)\n",
    "    $$\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i$$\n",
    "    $$\\sigma_{\\mathcal{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\mathcal{B}})^2$$\n",
    "1. 정규화 (Normalization)\n",
    "    - 배치 내에서 평균이 0, 분산이 1이 되도록 값을 조정 ($\\epsilon$은 안정성을 위한 작은 수)\n",
    "    $$\\hat{x}_i = \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}}\n",
    "1. $$스케일링 & 이동 (Scale & Shift)\n",
    "    - 정규화된 값이 데이터의 고유한 특징을 잃지 않도록 조정\n",
    "    - 학습 가능한 파라미터 감마($\\gamma$)와 베타($\\beta$) 사용 (채널/특성별로 존재)\n",
    "    $$y_i = \\gamma \\cdot \\hat{x}_i + \\beta$$\n",
    "\n",
    "Note: 학습(Train) 때는 위 계산을 매번 수행하지만, 추론(Inference/Eval) 때는 학습 중 구해놓은 '이동 평균(Moving Average)'을 사용한다는 점이 LayerNorm과 가장 큰 차이입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08107c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 512])\n",
      "torch.Size([4, 10, 512])\n",
      "torch.Size([4, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Batch Normalization\n",
    "# B, L, dim\n",
    "B, L, dim = 4, 10, 512\n",
    "x = torch.randn(B, L, dim)\n",
    "print(x.shape)\n",
    "\n",
    "mean_bn = torch.mean(x, dim=0, keepdim=True)\n",
    "var_bn = torch.var(x, dim=0, unbiased=False ,keepdim=True)\n",
    "std_bn = torch.sqrt(var_bn + 1e-6)\n",
    "\n",
    "x_norm = (x-mean_bn) / std_bn\n",
    "print(x_norm.shape)\n",
    "\n",
    "gamma = torch.ones(dim)\n",
    "beta = torch.zeros(dim)\n",
    "\n",
    "y = x_norm * gamma + beta\n",
    "\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa1dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
