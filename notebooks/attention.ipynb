{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db04169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c6891",
   "metadata": {},
   "source": [
    "### Scaled-dot product Attention\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b1b9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4887)\n",
      "tensor(0.1545)\n"
     ]
    }
   ],
   "source": [
    "Q = torch.randn(10)\n",
    "K = torch.randn(10)\n",
    "\n",
    "print(Q @ K)\n",
    "# Q @ K -> 차원수에 비례해서 커짐\n",
    "# 값이 커지면 Softmax 기울이가 0에 가까워짐 -> Saturation 현상 발생\n",
    "# sqrt(d_k)로 분산1을 유지 -> Scaling\n",
    "print(Q @ K / (Q.shape[0] ** 0.5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5ab52a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확장\n",
    "# (L,d) -> 문장의 길이가 L, 하나의 단어의 표현이 d 차원\n",
    "L = 10\n",
    "d = 3\n",
    "Q_matrix = torch.randn(L, d)\n",
    "K_matrix = torch.randn(L, d)\n",
    "V_matrix = torch.randn(L, d)\n",
    "\n",
    "scores = Q_matrix @ K_matrix.T\n",
    "print(scores.shape)\\\n",
    "\n",
    "\n",
    "scaled_scores = scores / (d**0.5)\n",
    "\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "output = attention_weights @ V_matrix\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "663d7816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 형태: torch.Size([2, 10, 3])\n",
      "가중치 형태: torch.Size([2, 10, 10])\n",
      "출력 형태: torch.Size([2, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "B,L,d = 2, 10,  3\n",
    "\n",
    "Q = torch.randn(B, L, d)\n",
    "K = torch.randn(B, L, d)\n",
    "V = torch.randn(B, L, d)\n",
    "\n",
    "# (B, L, d) (B, L, d) -> (B, L, L)\n",
    "attention_scores = Q @ K.transpose(-2,-1)\n",
    "\n",
    "attention_weights = F.softmax(attention_scores / (d**0.5), dim=-1)\n",
    "\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(f\"입력 형태: {Q.shape}\")  # torch.Size([2, 3, 4])\n",
    "print(f\"가중치 형태: {attention_weights.shape}\") # torch.Size([2, 3, 3])\n",
    "print(f\"출력 형태: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "373088fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, L, d_model = 4, 10, 512 \n",
    "d_k = 64\n",
    "\n",
    "X = torch.randn(B, L, d_model)\n",
    "\n",
    "W_q = torch.randn(d_model, d_k, requires_grad=True)\n",
    "W_k = torch.randn(d_model, d_k, requires_grad=True)\n",
    "W_v = torch.randn(d_model, d_k, requires_grad=True)\n",
    "\n",
    "Q = X @ W_q\n",
    "K = X @ W_k\n",
    "V = X @ W_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85205c60",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "- Encoder Layer\n",
    "    - Multi-Head Attention\n",
    "    - PositionwiseFeedForward\n",
    "    - LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77d9d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4185627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        heads: int,\n",
    "        d_ff: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        # Multi-Head Attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, heads)\n",
    "\n",
    "        # Feed Forward\n",
    "        self.feed_forward = PositionalwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # Layer Norm\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # 1\n",
    "        # Attention\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        # Add & Norm\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        # 2\n",
    "        # Sub-layer\n",
    "        ffn_output = self.feed_forward(x)\n",
    "        # Add & Norm\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df933d",
   "metadata": {},
   "source": [
    "### 초기화 공부 필요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model: int , heads: int):\n",
    "        # 가중치\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_head = d_model // heads\n",
    "\n",
    "        self.w_q = torch.randn(d_model, d_model, requires_grad=True) / math.sqrt(d_model)\n",
    "        self.w_k = torch.randn(d_model, d_model, requires_grad=True) / math.sqrt(d_model)\n",
    "        self.w_v = torch.randn(d_model, d_model, requires_grad=True) / math.sqrt(d_model)\n",
    "\n",
    "        self.w_o = torch.randn(d_model, d_model, requires_grad=True) / math.sqrt(d_model)\n",
    "\n",
    "    def __call__(self, query, key, value):\n",
    "        return self.forward(query, key, value)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # [B,L,dim]\n",
    "        Q = torch.matmul(query, self.w_q)\n",
    "        K = torch.matmul(key, self.w_k)\n",
    "        V = torch.matmul(value, self.w_v)\n",
    "\n",
    "        # [B,L,dim] -> [B,L,heads,d_head] -> [B,heads,L,d_head]\n",
    "        Q = Q.view(batch_size, -1, self.heads, self.d_head).transpose(1,2)\n",
    "        K = K.view(batch_size, -1, self.heads, self.d_head).transpose(1,2)\n",
    "        V = V.view(batch_size, -1, self.heads, self.d_head).transpose(1,2)\n",
    "\n",
    "        attention_scores = torch.matmul(Q,K.transpose(-2,-1)) / (self.d_head ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # [B,heads,L,d_head] -> [B,L,heads,d_head] -> [B,L,heads*d_head]\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        output = torch.matmul(context, self.w_o)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.w_q, self.w_k, self.w_v, self.w_o]\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "429d6d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MultiHeadAttention at 0x11420b290>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model, heads = 512, 8\n",
    "B, L = 10, 16\n",
    "\n",
    "model = MultiHeadAttention(d_model, heads)\n",
    "X = torch.randn(B, L, d_model)\n",
    "\n",
    "target = torch.randn(B, L, d_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd0f895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.tensor([\n",
    "    [10.5, 2.3, 1.1],  # \"Hi\"가 \"Hi\", \"PAD\", \"PAD\"를 본 점수\n",
    "    [2.3,  0.5, 0.2],\n",
    "    [1.1,  0.2, 0.1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c569337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0500e+01, -1.0000e+09, -1.0000e+09],\n",
       "        [ 2.3000e+00, -1.0000e+09, -1.0000e+09],\n",
       "        [ 1.1000e+00, -1.0000e+09, -1.0000e+09]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([1, 0, 0])\n",
    "scores.masked_fill(mask == 0, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5cf07757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.864864864864865"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110000/7400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc39b6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.5467e-01, -2.2579e-03, -1.2866e+00],\n",
       "        [ 3.3274e-01,  1.8297e-01,  1.2852e+00],\n",
       "        [-2.1007e+00, -2.1623e-01,  3.8635e-01],\n",
       "        [-3.0317e-01,  1.6471e+00,  4.1609e-01],\n",
       "        [-7.2088e-02,  5.0962e-01, -3.1438e+00],\n",
       "        [-1.4600e+00, -3.5646e-01, -1.5728e+00],\n",
       "        [-3.7831e-02,  4.7514e-01, -4.3947e-01],\n",
       "        [ 4.9029e-01, -1.1372e-01, -1.1840e+00],\n",
       "        [-2.3882e-01, -9.1874e-02,  1.0300e+00],\n",
       "        [-9.0995e-01, -1.2539e-02,  9.3577e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(10,3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed6586e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        heads: int,\n",
    "    ):\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.head_dim = d_model // heads\n",
    "        self.training = True\n",
    "        \n",
    "        if d_model % heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by heads.\")\n",
    "        \n",
    "        self.w_q = torch.randn(d_model, d_model) / math.sqrt(d_model)\n",
    "        self.w_k = torch.randn(d_model, d_model) / math.sqrt(d_model)\n",
    "        self.w_v = torch.randn(d_model, d_model) / math.sqrt(d_model)\n",
    "        self.w_o = torch.randn(d_model, d_model) / math.sqrt(d_model)\n",
    "\n",
    "        self.w_q.requires_grad = True\n",
    "        self.w_k.requires_grad = True\n",
    "        self.w_v.requires_grad = True\n",
    "        self.w_o.requires_grad = True\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.forward(query, key, value, mask)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # [B,L,d_model] -> [B,L,d_model]\n",
    "        q = torch.matmul(query, self.w_q)\n",
    "        k = torch.matmul(key, self.w_k)\n",
    "        v = torch.matmul(value, self.w_v)\n",
    "\n",
    "        # [B,L,d_model] -> [B,L,heads,d_heads] -> [B,heads,L,d_heads]\n",
    "        q = q.view(batch_size, -1, self.heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_size, -1, self.heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, -1, self.heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # attention score\n",
    "        # [B,heads,L,d_heads] @ [B,heads,d_heads,L] -> [B,heads,L,L]\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Padding mask\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # attention weights\n",
    "        # [B,heads,L,L] @ [B,heads,L,d_heads] -> [B,heads,L,d_heads]\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # Concationate\n",
    "        # [B,heads,L,d_heads] -> [B,L,heads,d_heads] -> [B,L,d_model]\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        output = torch.matmul(context, self.w_o)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.w_q, self.w_k, self.w_v, self.w_o]\n",
    "    \n",
    "    def train(self) -> None:\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self) -> None:\n",
    "        self.training = False\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        self.w_q = self.w_q.to(device)\n",
    "        self.w_k = self.w_k.to(device)\n",
    "        self.w_v = self.w_v.to(device)\n",
    "        self.w_o = self.w_o.to(device)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "570f4b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Example\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "# Activate module\n",
    "\n",
    "layer_norm(embedding).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d79e9a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9073e-06, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(layer_norm(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e421db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
