{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f38c74",
   "metadata": {},
   "source": [
    "## Masked Multi-Head Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
    "- $M$: Look-Ahead Mask\n",
    "    - $M = \\begin{bmatrix} \n",
    "0 & -\\infty & -\\infty & -\\infty \\\\\n",
    "0 & 0 & -\\infty & -\\infty \\\\\n",
    "0 & 0 & 0 & -\\infty \\\\\n",
    "0 & 0 & 0 & 0 \n",
    "\\end{bmatrix}$\n",
    "    - $0$인 부분: 볼 수 있는 과거와 현재\n",
    "    - $-\\infty$인 부분: 볼 수 없는 미래(Softmax를 취하면 $e^{-\\infty} \\approx 0$이 되어 확률이 사라짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2fb5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c25818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [L, dim]\n",
    "L , dim = 10, 64\n",
    "x = torch.randn(L, dim, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# [L ,L]\n",
    "mask = torch.ones(L, L)\n",
    "torch.tril(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tril: Lower triangle\n",
    "    # 밑 부분만 남김\n",
    "# torch.triu: Upper triangle\n",
    "    # 아래 부분만 남김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eaf599d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_mask = torch.triu(torch.ones(L, L), diagonal=1)\n",
    "add_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa1aa341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_mask.masked_fill(add_mask == 1, float('-inf'))\n",
    "add_mask.masked_fill(add_mask == 0, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a25078ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(L,L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "991559be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(L,L))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4cde08",
   "metadata": {},
   "source": [
    "### 방법 1\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}(\\text{masked\\_fill}(\\frac{QK^T}{\\sqrt{d_k}}, \\text{Mask}==0, -\\infty))V$$\n",
    "\n",
    "1. Binary Mask\n",
    "    - 1(True): 유효한 영역\n",
    "    - 0(False): 가려야 할 영역\n",
    "1. Mapping\n",
    "    - `masked_fill` 사용\n",
    "    - 마스크가 0인 위치의 값만 강제로 $-\\infty$로 덮어씌우고, 나머지 값은 건드리지 않고 그대로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f308fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0155e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [ 1.5688e+00,  5.9407e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [-1.9798e-01,  4.3771e-01, -1.4651e+00, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [ 6.7507e-01, -1.8383e+00,  8.3622e-01, -2.4066e-02, -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [-3.5347e-01,  7.4248e-01, -1.0590e+00,  4.7114e-01, -7.9914e-01,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [-1.3309e+00,  7.7494e-01, -1.1387e+00,  1.9499e-01,  8.2780e-02,\n",
       "          4.9165e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [-5.0612e-01,  1.9958e-01, -1.0390e+00, -3.3517e-02,  9.5005e-01,\n",
       "         -1.1797e+00, -2.0787e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [-3.8566e-01,  1.0433e+00,  4.6486e-01, -1.1823e-01, -3.8924e-01,\n",
       "         -2.5688e-01,  2.0777e+00, -1.1470e+00, -1.0000e+09, -1.0000e+09],\n",
       "        [ 6.3118e-01,  2.3730e+00, -3.0784e-01, -1.5227e-01, -5.0261e-01,\n",
       "         -7.8288e-01,  9.1539e-01,  7.9645e-01, -2.0569e-01, -1.0000e+09],\n",
       "        [ 1.4083e+00,  1.5775e-01,  6.5191e-01, -5.6558e-01, -4.2663e-01,\n",
       "          1.4069e-01, -2.0287e+00,  3.5152e-01, -4.7987e-01, -1.0498e+00]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 방식 1\n",
    "L, dim = 10, 64\n",
    "# x를 softmax 전이라고 생각\n",
    "x = torch.randn(L, L, requires_grad=True)\n",
    "causal = torch.tril(torch.ones(L,L, dtype=torch.long))\n",
    "print(causal)\n",
    "x.masked_fill(causal == 0, -1e9)\n",
    "# 이후는 동일함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3df5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 1, 10])\n",
      "torch.Size([4, 8, 10, 10])\n",
      "tensor([[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          6.1687e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -7.2560e-01,  9.6310e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          3.4232e-01, -3.3045e-03, -2.7576e-01, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -2.0086e-01, -1.6038e+00,  1.7085e-01,  3.5913e-01, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -2.4251e-01, -4.3429e-01, -2.9565e-01, -8.8700e-01,  1.4908e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "B, L, dim, H = 4, 10, 64, 8\n",
    "# [B, L, dim]\n",
    "x = torch.randn(B, H, L, L, dtype=torch.float, requires_grad=True)\n",
    "input_ids = torch.tensor([\n",
    "    [0,0,0,0,0, 5,6,7,8,9],\n",
    "    [0,0,3,4,5, 6,7,8,9,1],\n",
    "    [2,3,0,0,0, 0,0,0,0,0],\n",
    "    [1,2,3,4,5, 6,0,0,0,0],\n",
    "], dtype=torch.long)\n",
    "\n",
    "# padding mask\n",
    "# [B, L] -> [B, 1, 1, L]\n",
    "pad_mask = (input_ids != 0)[:, None, None, :]\n",
    "print(pad_mask.shape)\n",
    "\n",
    "# causal_mask\n",
    "# [L, L] -> [1, 1, L, L]\n",
    "causal = torch.tril(torch.ones(L, L, dtype=torch.long))\n",
    "causal = causal[None, None, :, :]\n",
    "\n",
    "# 결합\n",
    "# padding: [B, 1, 1, L]\n",
    "# causal: [1, 1, L, L]\n",
    "# [B, 1, 1, L] -> [B, 1, L, L]\n",
    "# [1, 1, L, L] -> [B, 1, L, L]\n",
    "mask = pad_mask * causal\n",
    "\n",
    "output = x.masked_fill(mask == 0, -1e9)\n",
    "print(output.shape)\n",
    "print(output[0][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55212a",
   "metadata": {},
   "source": [
    "### 방법2\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M_{\\text{additive}}\\right)V$$\n",
    "\n",
    "1. Additive Mask 행렬\n",
    "    $- 0$: 유효한 영역 (값 보존)\n",
    "    - $-\\infty$ (또는 $-1e9$): 가려야 할 영역 (값 파괴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23928cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1055e-02, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [-1.4350e-01,  1.3683e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [ 1.5323e+00, -6.2992e-01, -1.7739e-01, -1.0000e+09, -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [ 4.2355e-01,  5.5030e-01,  7.4291e-01, -1.7069e-01, -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [ 7.6101e-03,  1.5022e-01,  7.8821e-01, -1.6962e+00, -7.8850e-01,\n",
       "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [-2.2091e-01, -5.7895e-01,  9.2281e-01, -1.9385e-01, -2.0747e-01,\n",
       "         -2.1172e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [-3.4514e-01,  6.9409e-01,  1.3448e-01,  7.5586e-01, -5.8678e-01,\n",
       "         -2.6284e-02,  2.7105e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "        [ 7.8139e-01, -4.6712e-01, -5.0278e-01,  2.9387e-01,  1.6903e+00,\n",
       "          7.3226e-01, -1.2013e+00,  7.3869e-01, -1.0000e+09, -1.0000e+09],\n",
       "        [-5.4685e-01,  2.7481e-01,  1.2219e+00, -6.1087e-02, -1.0426e+00,\n",
       "         -1.1847e-01, -2.5038e-01, -6.2398e-01,  1.7715e+00, -1.0000e+09],\n",
       "        [ 2.0884e+00,  8.3377e-01,  1.1901e+00, -8.9790e-01, -3.2267e-01,\n",
       "         -5.6918e-01, -1.4082e+00, -1.1455e+00,  2.7690e-01, -1.0806e+00]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(L, L, requires_grad=True)\n",
    "\n",
    "additive_mask = torch.zeros(L, L)\n",
    "\n",
    "mask_indices = torch.triu_indices(L, L, offset=1)\n",
    "additive_mask[mask_indices[0], mask_indices[1]] = -1e9\n",
    "additive_mask\n",
    "\n",
    "x + additive_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07941c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        heads: int,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        if d_model % heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by heads.\")\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.head_dim = d_model // heads\n",
    "        self.bias = bias\n",
    "        self.training = True\n",
    "\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        self.w_q = torch.randn(d_model, d_model) / self.scale\n",
    "        self.w_q.requires_grad_()\n",
    "        self.w_k = torch.randn(d_model, d_model) / self.scale\n",
    "        self.w_k.requires_grad_()\n",
    "        self.w_v = torch.randn(d_model, d_model) / self.scale\n",
    "        self.w_v.requires_grad_()\n",
    "        self.w_o = torch.randn(d_model, d_model) / self.scale\n",
    "        self.w_o.requires_grad_()\n",
    "\n",
    "        if bias:\n",
    "            self.b_q = torch.zeros(d_model, requires_grad=True)\n",
    "            self.b_k = torch.zeros(d_model, requires_grad=True)\n",
    "            self.b_v = torch.zeros(d_model, requires_grad=True)\n",
    "            self.b_o = torch.zeros(d_model, requires_grad=True)\n",
    "        else:\n",
    "            self.b_q = self.b_k = self.b_v = self.b_o = None  \n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.forward(query, key, value, key_padding_mask, attn_mask)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Q, K, V 생성 (각각은 [B, max_len, d_model])\n",
    "        # [B, max_len, d_model] * [d_model, d_model] -> [B, max_len, d_model]\n",
    "        q = torch.matmul(query, self.w_q)\n",
    "        if self.b_q is not None:\n",
    "            q = q + self.b_q\n",
    "        k = torch.matmul(key, self.w_k)\n",
    "        if self.b_k is not None:\n",
    "            k = k + self.b_k\n",
    "        v = torch.matmul(value, self.w_v)\n",
    "        if self.b_v is not None:\n",
    "            v = v + self.b_v\n",
    "        \n",
    "        # q,k,v를 multihead로 reshape\n",
    "        # [B, max_len, d_model] -> [B, max_len, heads, d_heads] -> [B, heads, max_len, d_heads]\n",
    "        q = q.view(batch_size, -1, self.heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_size, -1, self.heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, -1, self.heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # attention_score\n",
    "        # [B, heads, max_len, d_heads] @ [B, heads, d_heads, max_len] -> [B, heads, max_len, max_len]\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Mask\n",
    "        # [L_q, L_k] -> [1, 1, L_q, L_k]\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask[None, None, :, :]\n",
    "        \n",
    "        # padding\n",
    "        # [B,L] -> [B, 1, 1, L] \n",
    "        if key_padding_mask is not None:\n",
    "            if key_padding_mask.dim() == 2:\n",
    "                key_padding_mask = key_padding_mask[:, None, None, :]\n",
    "\n",
    "        if attn_mask is not None and key_padding_mask is not None:\n",
    "            mask = attn_mask * key_padding_mask\n",
    "        elif attn_mask is not None:\n",
    "            mask = attn_mask\n",
    "        elif key_padding_mask is not None:\n",
    "            mask = key_padding_mask\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # attention weights\n",
    "        # [B, heads, max_len, max_len] @ [B, heads, max_len, d_heads] -> [B, heads, max_len, d_heads]\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Concationate\n",
    "        # [B, heads, max_len, d_heads] -> [B, max_len, heads, d_heads] -> [B, max_len, d_model]\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        output = torch.matmul(context, self.w_o)\n",
    "        if self.b_o is not None:\n",
    "            output = output + self.b_o\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "        params = [self.w_q, self.w_k, self.w_v, self.w_o]\n",
    "        if self.b_q is not None:\n",
    "            params.extend([self.b_q, self.b_k, self.b_v, self.b_o])\n",
    "        return params\n",
    "    \n",
    "    def train(self, mode: bool = True):\n",
    "        self.training = mode\n",
    "        return self\n",
    "\n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "    \n",
    "    def to(self, device: torch.device):\n",
    "        self.w_q = self.w_q.to(device).detach().requires_grad_(True)\n",
    "        self.w_k = self.w_k.to(device).detach().requires_grad_(True)\n",
    "        self.w_v = self.w_v.to(device).detach().requires_grad_(True)\n",
    "        self.w_o = self.w_o.to(device).detach().requires_grad_(True)\n",
    "\n",
    "        if self.b_q is not None:\n",
    "            self.b_q = self.b_q.to(device).detach().requires_grad_(True)\n",
    "            self.b_k = self.b_k.to(device).detach().requires_grad_(True)\n",
    "            self.b_v = self.b_v.to(device).detach().requires_grad_(True)\n",
    "            self.b_o = self.b_o.to(device).detach().requires_grad_(True)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6baabb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4bad5808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape: torch.Size([2, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "B, Lq, Lk, d_model, heads = 2, 5, 7, 64, 8\n",
    "\n",
    "mha = MultiHeadAttention(d_model=d_model, heads=heads).to(device)\n",
    "\n",
    "q = torch.randn(B, Lq, d_model, device=device)\n",
    "k = torch.randn(B, Lk, d_model, device=device)\n",
    "v = torch.randn(B, Lk, d_model, device=device)\n",
    "\n",
    "out = mha(q, k, v)\n",
    "print(\"out shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "931ae17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_q grad is None? False | grad norm: 0.10621220618486404\n",
      "w_k grad is None? False | grad norm: 0.1277574896812439\n",
      "w_v grad is None? False | grad norm: 0.2637781798839569\n",
      "w_o grad is None? False | grad norm: 0.28294530510902405\n"
     ]
    }
   ],
   "source": [
    "B, L, d_model, heads = 2, 6, 64, 8\n",
    "mha = MultiHeadAttention(d_model=d_model, heads=heads).to(device)\n",
    "\n",
    "x = torch.randn(B, L, d_model, device=device)\n",
    "out = mha(x, x, x)\n",
    "\n",
    "loss = out.mean()\n",
    "mha.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "for name, p in zip([\"w_q\",\"w_k\",\"w_v\",\"w_o\"], [mha.w_q, mha.w_k, mha.w_v, mha.w_o]):\n",
    "    g = p.grad\n",
    "    print(name, \"grad is None?\", g is None, \"| grad norm:\", None if g is None else g.norm().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60c43bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_q changed abs-sum: 0.7707235813140869\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "B, L, d_model, heads = 2, 6, 64, 8\n",
    "mha = MultiHeadAttention(d_model=d_model, heads=heads).to(device)\n",
    "\n",
    "opt = optim.SGD(mha.parameters(), lr=1e-1)\n",
    "\n",
    "x = torch.randn(B, L, d_model, device=device)\n",
    "\n",
    "wq_before = mha.w_q.detach().clone()\n",
    "\n",
    "out = mha(x, x, x)\n",
    "loss = out.pow(2).mean()\n",
    "\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "diff = (mha.w_q.detach() - wq_before).abs().sum().item()\n",
    "print(\"w_q changed abs-sum:\", diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86c2dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff(out) abs-sum: 132.52188110351562\n"
     ]
    }
   ],
   "source": [
    "B, L, d_model, heads = 1, 6, 64, 8\n",
    "mha = MultiHeadAttention(d_model=d_model, heads=heads).to(device)\n",
    "\n",
    "x = torch.randn(B, L, d_model, device=device)\n",
    "\n",
    "# 마지막 3개 토큰을 pad(0)라고 치자\n",
    "key_padding_mask = torch.tensor([[1,1,1,0,0,0]], device=device)\n",
    "\n",
    "out_no_mask = mha(x, x, x, key_padding_mask=None, attn_mask=None)\n",
    "out_pad_mask = mha(x, x, x, key_padding_mask=key_padding_mask, attn_mask=None)\n",
    "\n",
    "print(\"diff(out) abs-sum:\", (out_no_mask - out_pad_mask).abs().sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ed02f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff(out) abs-sum: 149.92823791503906\n"
     ]
    }
   ],
   "source": [
    "B, L, d_model, heads = 1, 6, 64, 8\n",
    "mha = MultiHeadAttention(d_model=d_model, heads=heads).to(device)\n",
    "\n",
    "x = torch.randn(B, L, d_model, device=device)\n",
    "\n",
    "attn_mask = torch.tril(torch.ones(L, L, device=device))\n",
    "\n",
    "out_no_mask = mha(x, x, x, attn_mask=None)\n",
    "out_causal = mha(x, x, x, attn_mask=attn_mask)\n",
    "\n",
    "print(\"diff(out) abs-sum:\", (out_no_mask - out_causal).abs().sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f438c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape: torch.Size([2, 6, 64])\n",
      "out finite? True\n"
     ]
    }
   ],
   "source": [
    "B, L, d_model, heads = 2, 6, 64, 8\n",
    "mha = MultiHeadAttention(d_model=d_model, heads=heads).to(device)\n",
    "x = torch.randn(B, L, d_model, device=device)\n",
    "\n",
    "key_padding_mask = torch.tensor([\n",
    "    [1,1,1,1,0,0],\n",
    "    [1,1,0,0,0,0],\n",
    "], device=device)\n",
    "\n",
    "attn_mask = torch.tril(torch.ones(L, L, device=device))\n",
    "\n",
    "out = mha(x, x, x, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "print(\"out shape:\", out.shape)\n",
    "print(\"out finite?\", torch.isfinite(out).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17867297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
