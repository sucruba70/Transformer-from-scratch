{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1d2f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7213c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(num_embeddings=5, embedding_dim=3)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db66693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_fill_padding_idx_with_zero',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embedding_dim',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'max_norm',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'norm_type',\n",
       " 'num_embeddings',\n",
       " 'padding_idx',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'scale_grad_by_freq',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'sparse',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "742a7215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Parameter containing:\n",
      "tensor([[ 0.8124, -0.5914,  1.0241],\n",
      "        [ 0.5375, -0.7507,  0.2593],\n",
      "        [-2.0287, -1.6452,  1.5384],\n",
      "        [-0.5805, -0.5312,  1.0031],\n",
      "        [ 0.8015, -2.1029,  0.2422]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(emb.padding_idx)\n",
    "print(emb.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "313a3e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([0]).dtype)\n",
    "print(torch.Tensor([0]).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f125ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8124, -0.5914,  1.0241]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(emb(torch.tensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "205d9f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 256])\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([100])\n",
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0722e-02,  3.4651e-01, -1.6014e+00,  1.1328e+00,  3.7816e-01,\n",
       "         -1.9764e+00,  6.8071e-01, -3.7107e-01,  1.3613e+00,  2.1737e+00,\n",
       "          2.2640e+00,  1.1904e+00, -1.8112e+00,  1.4188e+00, -6.9237e-01,\n",
       "          4.5711e-01,  6.6513e-01, -6.5726e-01,  1.1107e+00,  1.0151e+00,\n",
       "         -1.3071e+00,  9.1559e-01,  1.2806e-01,  3.1776e-01,  9.5807e-01,\n",
       "         -1.8630e+00,  1.0862e+00,  1.0757e-01, -2.1369e+00, -1.3925e-01,\n",
       "          5.9510e-01, -2.0587e-01,  1.1864e+00,  1.4223e+00,  1.9868e-01,\n",
       "          7.1203e-01,  1.2306e+00,  6.2350e-05,  7.0175e-01,  1.9119e+00,\n",
       "          1.6196e+00, -1.6134e-01,  4.3097e-01, -2.8825e-01,  2.0788e-01,\n",
       "         -1.1368e+00, -9.2631e-01,  1.5338e-01,  1.6220e+00,  9.2390e-01,\n",
       "         -1.9136e+00,  1.1105e-01, -1.2947e+00,  3.7382e-01, -6.8803e-01,\n",
       "          1.1713e+00,  1.8557e+00,  1.9943e-01,  5.5493e-01,  1.3390e+00,\n",
       "         -1.0884e+00,  1.6349e+00, -9.2087e-01,  1.6598e+00, -4.1417e-01,\n",
       "          1.6856e+00, -8.0029e-01, -2.0695e+00, -2.2128e+00,  1.5906e+00,\n",
       "          3.1676e+00,  1.5752e-01, -8.6487e-01, -5.3740e-01,  8.0153e-03,\n",
       "         -1.7272e-01,  6.4925e-01, -1.7203e+00,  4.3694e-01,  1.1142e+00,\n",
       "          9.2669e-02, -1.3470e-01, -6.5447e-01,  2.0833e-01,  6.3027e-01,\n",
       "          1.6910e+00,  9.8312e-02, -9.9672e-02,  8.5375e-01, -1.0877e+00,\n",
       "         -2.4912e-01,  1.6744e+00,  1.8877e+00, -4.8027e-01, -1.3922e+00,\n",
       "         -2.3406e+00,  1.6115e-01, -7.3032e-02, -8.1437e-01,  1.0063e+00,\n",
       "         -1.0608e+00, -7.2953e-01, -6.2693e-01, -8.1419e-01, -9.8922e-01,\n",
       "         -1.0785e+00, -2.7004e-01,  1.3095e+00,  1.0249e-01,  1.7552e+00,\n",
       "          1.1545e+00,  1.6778e+00,  2.9411e-02,  5.2241e-01,  1.4666e-01,\n",
       "         -2.3016e-01, -2.8367e+00, -4.3944e-01, -2.3788e-01, -1.0164e-01,\n",
       "         -4.6002e-01, -7.4652e-01, -1.2645e-01, -4.1214e-01,  1.2528e+00,\n",
       "         -2.5812e-01,  7.3484e-01,  1.7251e+00, -4.5457e-01,  9.9632e-01,\n",
       "          1.8035e+00, -5.0205e-01, -3.2196e-02, -1.3162e-01, -2.9743e-01,\n",
       "         -8.2915e-01,  8.9058e-01,  5.4522e-02,  4.5378e-01, -1.6601e+00,\n",
       "          2.9884e+00,  1.6721e-01,  8.5979e-01,  6.0044e-01, -6.2896e-01,\n",
       "          2.9401e+00,  1.1175e+00, -2.5392e-01,  2.2695e-01, -5.8990e-01,\n",
       "         -3.5500e-01, -1.1445e-01,  9.9819e-01, -1.0072e+00,  5.7081e-01,\n",
       "         -7.6980e-01,  4.1945e-01, -4.4883e-01, -1.2875e+00, -1.3192e-01,\n",
       "          5.7632e-01, -3.4259e-01,  1.8208e-01,  1.0872e+00, -4.1835e-01,\n",
       "         -5.2926e-01, -1.3200e+00, -9.6056e-01,  1.0273e+00, -1.2040e-01,\n",
       "         -4.7573e-01,  1.0468e+00, -2.3773e-01,  1.5638e-01,  6.6950e-01,\n",
       "          3.4057e-01,  1.3317e+00,  1.7555e+00,  3.8658e-01, -3.7566e-01,\n",
       "         -3.2197e-01,  3.0512e-01, -7.5336e-01, -7.8703e-01, -8.3279e-01,\n",
       "          3.4272e-01,  2.5019e-01, -1.4999e+00,  2.0294e+00,  4.4727e-01,\n",
       "         -3.1159e-01,  3.7592e-01,  6.1591e-01,  2.1911e+00,  8.7429e-01,\n",
       "          1.3428e+00,  5.9254e-02, -2.4504e-01,  4.0056e-01, -1.4041e+00,\n",
       "          7.4663e-01, -2.5148e-01,  8.6675e-01,  4.8161e-01,  1.1753e+00,\n",
       "          2.4268e-02, -1.5316e+00, -5.3781e-01,  1.2727e+00, -2.7057e-01,\n",
       "         -1.7865e+00,  2.7865e-01,  1.3834e+00, -7.4834e-01,  2.0775e-01,\n",
       "          5.3065e-01,  4.9025e-01,  5.9305e-01,  1.8891e-01,  6.3636e-01,\n",
       "         -1.3938e+00,  1.5308e-01,  1.1917e-01,  8.2456e-01,  3.8008e-01,\n",
       "          6.6065e-01, -4.7488e-01, -9.1133e-01, -2.6764e-01, -5.2398e-01,\n",
       "          8.2610e-01,  9.8679e-01, -3.1747e-01,  1.0006e+00,  4.5256e-01,\n",
       "         -6.7316e-01, -1.0780e+00,  6.7026e-01, -1.3633e+00, -5.2877e-02,\n",
       "          1.9116e-01,  1.1407e+00,  2.7239e-01,  6.6360e-01,  6.7545e-01,\n",
       "          2.9992e-01,  7.6298e-01, -2.9637e-01, -7.4312e-01, -5.9201e-02,\n",
       "          1.1635e-01, -6.0266e-01, -5.1037e-01, -2.5598e-01, -1.0969e+00,\n",
       "         -7.4855e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1. W 만들기\n",
    "# [Vocab_size, dim]\n",
    "vocab_size, dim = 100, 256\n",
    "W = torch.randn(vocab_size, dim, requires_grad=True)\n",
    "print(W.shape)\n",
    "\n",
    "### 2. 동작 과정\n",
    "# 2-1. 인덱스 k. 단어 0~99 중 5번째 index\n",
    "k = 5\n",
    "# 2-2. one-hot vector로 변환\n",
    "k_vector = torch.zeros(vocab_size)\n",
    "k_vector[k] = 1\n",
    "print(k_vector)\n",
    "print(k_vector.shape)\n",
    "k_vector = k_vector.unsqueeze(0)\n",
    "print(k_vector.shape)\n",
    "\n",
    "# 2-3. 행렬곱으로 W_5의 행만 추출\n",
    "torch.matmul(k_vector, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5f51507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "tensor([[-3.0722e-02,  3.4651e-01, -1.6014e+00,  1.1328e+00,  3.7816e-01,\n",
      "         -1.9764e+00,  6.8071e-01, -3.7107e-01,  1.3613e+00,  2.1737e+00,\n",
      "          2.2640e+00,  1.1904e+00, -1.8112e+00,  1.4188e+00, -6.9237e-01,\n",
      "          4.5711e-01,  6.6513e-01, -6.5726e-01,  1.1107e+00,  1.0151e+00,\n",
      "         -1.3071e+00,  9.1559e-01,  1.2806e-01,  3.1776e-01,  9.5807e-01,\n",
      "         -1.8630e+00,  1.0862e+00,  1.0757e-01, -2.1369e+00, -1.3925e-01,\n",
      "          5.9510e-01, -2.0587e-01,  1.1864e+00,  1.4223e+00,  1.9868e-01,\n",
      "          7.1203e-01,  1.2306e+00,  6.2350e-05,  7.0175e-01,  1.9119e+00,\n",
      "          1.6196e+00, -1.6134e-01,  4.3097e-01, -2.8825e-01,  2.0788e-01,\n",
      "         -1.1368e+00, -9.2631e-01,  1.5338e-01,  1.6220e+00,  9.2390e-01,\n",
      "         -1.9136e+00,  1.1105e-01, -1.2947e+00,  3.7382e-01, -6.8803e-01,\n",
      "          1.1713e+00,  1.8557e+00,  1.9943e-01,  5.5493e-01,  1.3390e+00,\n",
      "         -1.0884e+00,  1.6349e+00, -9.2087e-01,  1.6598e+00, -4.1417e-01,\n",
      "          1.6856e+00, -8.0029e-01, -2.0695e+00, -2.2128e+00,  1.5906e+00,\n",
      "          3.1676e+00,  1.5752e-01, -8.6487e-01, -5.3740e-01,  8.0153e-03,\n",
      "         -1.7272e-01,  6.4925e-01, -1.7203e+00,  4.3694e-01,  1.1142e+00,\n",
      "          9.2669e-02, -1.3470e-01, -6.5447e-01,  2.0833e-01,  6.3027e-01,\n",
      "          1.6910e+00,  9.8312e-02, -9.9672e-02,  8.5375e-01, -1.0877e+00,\n",
      "         -2.4912e-01,  1.6744e+00,  1.8877e+00, -4.8027e-01, -1.3922e+00,\n",
      "         -2.3406e+00,  1.6115e-01, -7.3032e-02, -8.1437e-01,  1.0063e+00,\n",
      "         -1.0608e+00, -7.2953e-01, -6.2693e-01, -8.1419e-01, -9.8922e-01,\n",
      "         -1.0785e+00, -2.7004e-01,  1.3095e+00,  1.0249e-01,  1.7552e+00,\n",
      "          1.1545e+00,  1.6778e+00,  2.9411e-02,  5.2241e-01,  1.4666e-01,\n",
      "         -2.3016e-01, -2.8367e+00, -4.3944e-01, -2.3788e-01, -1.0164e-01,\n",
      "         -4.6002e-01, -7.4652e-01, -1.2645e-01, -4.1214e-01,  1.2528e+00,\n",
      "         -2.5812e-01,  7.3484e-01,  1.7251e+00, -4.5457e-01,  9.9632e-01,\n",
      "          1.8035e+00, -5.0205e-01, -3.2196e-02, -1.3162e-01, -2.9743e-01,\n",
      "         -8.2915e-01,  8.9058e-01,  5.4522e-02,  4.5378e-01, -1.6601e+00,\n",
      "          2.9884e+00,  1.6721e-01,  8.5979e-01,  6.0044e-01, -6.2896e-01,\n",
      "          2.9401e+00,  1.1175e+00, -2.5392e-01,  2.2695e-01, -5.8990e-01,\n",
      "         -3.5500e-01, -1.1445e-01,  9.9819e-01, -1.0072e+00,  5.7081e-01,\n",
      "         -7.6980e-01,  4.1945e-01, -4.4883e-01, -1.2875e+00, -1.3192e-01,\n",
      "          5.7632e-01, -3.4259e-01,  1.8208e-01,  1.0872e+00, -4.1835e-01,\n",
      "         -5.2926e-01, -1.3200e+00, -9.6056e-01,  1.0273e+00, -1.2040e-01,\n",
      "         -4.7573e-01,  1.0468e+00, -2.3773e-01,  1.5638e-01,  6.6950e-01,\n",
      "          3.4057e-01,  1.3317e+00,  1.7555e+00,  3.8658e-01, -3.7566e-01,\n",
      "         -3.2197e-01,  3.0512e-01, -7.5336e-01, -7.8703e-01, -8.3279e-01,\n",
      "          3.4272e-01,  2.5019e-01, -1.4999e+00,  2.0294e+00,  4.4727e-01,\n",
      "         -3.1159e-01,  3.7592e-01,  6.1591e-01,  2.1911e+00,  8.7429e-01,\n",
      "          1.3428e+00,  5.9254e-02, -2.4504e-01,  4.0056e-01, -1.4041e+00,\n",
      "          7.4663e-01, -2.5148e-01,  8.6675e-01,  4.8161e-01,  1.1753e+00,\n",
      "          2.4268e-02, -1.5316e+00, -5.3781e-01,  1.2727e+00, -2.7057e-01,\n",
      "         -1.7865e+00,  2.7865e-01,  1.3834e+00, -7.4834e-01,  2.0775e-01,\n",
      "          5.3065e-01,  4.9025e-01,  5.9305e-01,  1.8891e-01,  6.3636e-01,\n",
      "         -1.3938e+00,  1.5308e-01,  1.1917e-01,  8.2456e-01,  3.8008e-01,\n",
      "          6.6065e-01, -4.7488e-01, -9.1133e-01, -2.6764e-01, -5.2398e-01,\n",
      "          8.2610e-01,  9.8679e-01, -3.1747e-01,  1.0006e+00,  4.5256e-01,\n",
      "         -6.7316e-01, -1.0780e+00,  6.7026e-01, -1.3633e+00, -5.2877e-02,\n",
      "          1.9116e-01,  1.1407e+00,  2.7239e-01,  6.6360e-01,  6.7545e-01,\n",
      "          2.9992e-01,  7.6298e-01, -2.9637e-01, -7.4312e-01, -5.9201e-02,\n",
      "          1.1635e-01, -6.0266e-01, -5.1037e-01, -2.5598e-01, -1.0969e+00,\n",
      "         -7.4855e-01]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "k_vector = torch.nn.functional.one_hot(torch.tensor([k]), num_classes=vocab_size)\n",
    "print(k_vector.dtype)\n",
    "k_vector = k_vector.float()\n",
    "\n",
    "print(torch.matmul(k_vector, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebab3670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0722e-02,  3.4651e-01, -1.6014e+00,  1.1328e+00,  3.7816e-01,\n",
       "        -1.9764e+00,  6.8071e-01, -3.7107e-01,  1.3613e+00,  2.1737e+00,\n",
       "         2.2640e+00,  1.1904e+00, -1.8112e+00,  1.4188e+00, -6.9237e-01,\n",
       "         4.5711e-01,  6.6513e-01, -6.5726e-01,  1.1107e+00,  1.0151e+00,\n",
       "        -1.3071e+00,  9.1559e-01,  1.2806e-01,  3.1776e-01,  9.5807e-01,\n",
       "        -1.8630e+00,  1.0862e+00,  1.0757e-01, -2.1369e+00, -1.3925e-01,\n",
       "         5.9510e-01, -2.0587e-01,  1.1864e+00,  1.4223e+00,  1.9868e-01,\n",
       "         7.1203e-01,  1.2306e+00,  6.2350e-05,  7.0175e-01,  1.9119e+00,\n",
       "         1.6196e+00, -1.6134e-01,  4.3097e-01, -2.8825e-01,  2.0788e-01,\n",
       "        -1.1368e+00, -9.2631e-01,  1.5338e-01,  1.6220e+00,  9.2390e-01,\n",
       "        -1.9136e+00,  1.1105e-01, -1.2947e+00,  3.7382e-01, -6.8803e-01,\n",
       "         1.1713e+00,  1.8557e+00,  1.9943e-01,  5.5493e-01,  1.3390e+00,\n",
       "        -1.0884e+00,  1.6349e+00, -9.2087e-01,  1.6598e+00, -4.1417e-01,\n",
       "         1.6856e+00, -8.0029e-01, -2.0695e+00, -2.2128e+00,  1.5906e+00,\n",
       "         3.1676e+00,  1.5752e-01, -8.6487e-01, -5.3740e-01,  8.0153e-03,\n",
       "        -1.7272e-01,  6.4925e-01, -1.7203e+00,  4.3694e-01,  1.1142e+00,\n",
       "         9.2669e-02, -1.3470e-01, -6.5447e-01,  2.0833e-01,  6.3027e-01,\n",
       "         1.6910e+00,  9.8312e-02, -9.9672e-02,  8.5375e-01, -1.0877e+00,\n",
       "        -2.4912e-01,  1.6744e+00,  1.8877e+00, -4.8027e-01, -1.3922e+00,\n",
       "        -2.3406e+00,  1.6115e-01, -7.3032e-02, -8.1437e-01,  1.0063e+00,\n",
       "        -1.0608e+00, -7.2953e-01, -6.2693e-01, -8.1419e-01, -9.8922e-01,\n",
       "        -1.0785e+00, -2.7004e-01,  1.3095e+00,  1.0249e-01,  1.7552e+00,\n",
       "         1.1545e+00,  1.6778e+00,  2.9411e-02,  5.2241e-01,  1.4666e-01,\n",
       "        -2.3016e-01, -2.8367e+00, -4.3944e-01, -2.3788e-01, -1.0164e-01,\n",
       "        -4.6002e-01, -7.4652e-01, -1.2645e-01, -4.1214e-01,  1.2528e+00,\n",
       "        -2.5812e-01,  7.3484e-01,  1.7251e+00, -4.5457e-01,  9.9632e-01,\n",
       "         1.8035e+00, -5.0205e-01, -3.2196e-02, -1.3162e-01, -2.9743e-01,\n",
       "        -8.2915e-01,  8.9058e-01,  5.4522e-02,  4.5378e-01, -1.6601e+00,\n",
       "         2.9884e+00,  1.6721e-01,  8.5979e-01,  6.0044e-01, -6.2896e-01,\n",
       "         2.9401e+00,  1.1175e+00, -2.5392e-01,  2.2695e-01, -5.8990e-01,\n",
       "        -3.5500e-01, -1.1445e-01,  9.9819e-01, -1.0072e+00,  5.7081e-01,\n",
       "        -7.6980e-01,  4.1945e-01, -4.4883e-01, -1.2875e+00, -1.3192e-01,\n",
       "         5.7632e-01, -3.4259e-01,  1.8208e-01,  1.0872e+00, -4.1835e-01,\n",
       "        -5.2926e-01, -1.3200e+00, -9.6056e-01,  1.0273e+00, -1.2040e-01,\n",
       "        -4.7573e-01,  1.0468e+00, -2.3773e-01,  1.5638e-01,  6.6950e-01,\n",
       "         3.4057e-01,  1.3317e+00,  1.7555e+00,  3.8658e-01, -3.7566e-01,\n",
       "        -3.2197e-01,  3.0512e-01, -7.5336e-01, -7.8703e-01, -8.3279e-01,\n",
       "         3.4272e-01,  2.5019e-01, -1.4999e+00,  2.0294e+00,  4.4727e-01,\n",
       "        -3.1159e-01,  3.7592e-01,  6.1591e-01,  2.1911e+00,  8.7429e-01,\n",
       "         1.3428e+00,  5.9254e-02, -2.4504e-01,  4.0056e-01, -1.4041e+00,\n",
       "         7.4663e-01, -2.5148e-01,  8.6675e-01,  4.8161e-01,  1.1753e+00,\n",
       "         2.4268e-02, -1.5316e+00, -5.3781e-01,  1.2727e+00, -2.7057e-01,\n",
       "        -1.7865e+00,  2.7865e-01,  1.3834e+00, -7.4834e-01,  2.0775e-01,\n",
       "         5.3065e-01,  4.9025e-01,  5.9305e-01,  1.8891e-01,  6.3636e-01,\n",
       "        -1.3938e+00,  1.5308e-01,  1.1917e-01,  8.2456e-01,  3.8008e-01,\n",
       "         6.6065e-01, -4.7488e-01, -9.1133e-01, -2.6764e-01, -5.2398e-01,\n",
       "         8.2610e-01,  9.8679e-01, -3.1747e-01,  1.0006e+00,  4.5256e-01,\n",
       "        -6.7316e-01, -1.0780e+00,  6.7026e-01, -1.3633e+00, -5.2877e-02,\n",
       "         1.9116e-01,  1.1407e+00,  2.7239e-01,  6.6360e-01,  6.7545e-01,\n",
       "         2.9992e-01,  7.6298e-01, -2.9637e-01, -7.4312e-01, -5.9201e-02,\n",
       "         1.1635e-01, -6.0266e-01, -5.1037e-01, -2.5598e-01, -1.0969e+00,\n",
       "        -7.4855e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c602d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4, 256])\n",
      "tensor([[-0.0307,  0.3465, -1.6014,  ..., -0.2560, -1.0969, -0.7485],\n",
      "        [-0.4187,  1.2387,  0.3733,  ...,  1.2210,  0.3334,  0.6010],\n",
      "        [-0.0307,  0.3465, -1.6014,  ..., -0.2560, -1.0969, -0.7485],\n",
      "        [-1.0016,  1.0505, -1.5956,  ..., -0.7479,  1.1884,  1.4126]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### 차원 확장\n",
    "# [L]\n",
    "ids = torch.tensor([5, 2, 5, 99], dtype=torch.long)\n",
    "print(ids.shape)\n",
    "print(W[ids].shape)\n",
    "print(W[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "901ad446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0307,  0.3465, -1.6014,  ..., -0.2560, -1.0969, -0.7485],\n",
       "        [-0.4187,  1.2387,  0.3733,  ...,  1.2210,  0.3334,  0.6010],\n",
       "        [-0.0307,  0.3465, -1.6014,  ..., -0.2560, -1.0969, -0.7485],\n",
       "        [-1.0016,  1.0505, -1.5956,  ..., -0.7479,  1.1884,  1.4126]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = torch.tensor([5, 2, 5, 99], dtype=torch.long)\n",
    "one_hot_ids = torch.nn.functional.one_hot(ids, num_classes=vocab_size).to(W.dtype)\n",
    "print(one_hot_ids.shape)\n",
    "\n",
    "torch.matmul(one_hot_ids, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62cace41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 256])\n",
      "tensor([[[-0.2622, -0.0513, -1.0129,  ...,  1.9487, -0.0412, -0.9401],\n",
      "         [ 0.6715, -0.3268,  0.2919,  ...,  0.4442, -0.9443, -1.4110],\n",
      "         [-0.5745,  0.0285,  0.6114,  ..., -0.3304, -0.0023,  1.7851],\n",
      "         [-1.3889, -1.8623,  0.5936,  ...,  0.0299,  0.4371,  0.6816]],\n",
      "\n",
      "        [[ 1.4983,  0.9322, -0.1769,  ..., -0.6162,  0.5138,  0.6981],\n",
      "         [ 0.4030, -1.0858, -0.8338,  ...,  0.1925, -0.9338,  1.1017],\n",
      "         [-0.7258,  1.9101,  0.7005,  ...,  0.7185,  1.5096,  0.3996],\n",
      "         [ 1.4983,  0.9322, -0.1769,  ..., -0.6162,  0.5138,  0.6981]]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# [B, L]\n",
    "B, L = 2, 4\n",
    "ids = torch.randint(0, vocab_size, (B,L), dtype=torch.long)\n",
    "print(ids.shape)\n",
    "print(W[ids].shape)\n",
    "print(W[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79a8ca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/56/h6vkkmvd0zzcr59jvcgzr4y40000gn/T/ipykernel_20850/938295077.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:497.)\n",
      "  print(w.grad)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(3,3) / math.sqrt(3)\n",
    "w.requires_grad = True\n",
    "print(w.is_leaf)\n",
    "\n",
    "w = w.to('mps')\n",
    "print(w.is_leaf)\n",
    "\n",
    "loss = (w**2).sum()\n",
    "loss.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5cb0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: Optional[int] = None,\n",
    "    ):\n",
    "        if padding_idx is not None:\n",
    "            if not (0 <= padding_idx < num_embeddings):\n",
    "                raise ValueError(\"padding_idx must be in [0, num_embeddings)\")\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.training = True\n",
    "\n",
    "        self.scale = math.sqrt(embedding_dim)\n",
    "        self.weight = torch.randn(num_embeddings, embedding_dim) / self.scale\n",
    "        self.weight.requires_grad_()\n",
    "\n",
    "        if self.padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight[self.padding_idx].zero_()\n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # [B,L] -> [B,L,embedding_dim]\n",
    "        return self.weight[x]\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "    \n",
    "    def train(self, mode: bool = True):\n",
    "        self.training = mode\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        self.weight = self.weight.to(device).detach().requires_grad_(True)\n",
    "        return self\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        if self.weight.grad is not None:\n",
    "            self.weight.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "48490364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "torch.Size([1, 4, 64])\n",
      "0.0\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 4, 64])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "### Test\n",
    "my_emb = Embedding(\n",
    "    num_embeddings=100,\n",
    "    embedding_dim=64,\n",
    "    padding_idx = 0\n",
    ")\n",
    "\n",
    "my_input_ids = torch.tensor([[1, 52, 73, 0]]) \n",
    "my_output = my_emb(my_input_ids)\n",
    "\n",
    "print(my_input_ids.shape)\n",
    "print(my_output.shape)\n",
    "print(my_output[0,3].sum().item())\n",
    "\n",
    "emb = nn.Embedding(\n",
    "    num_embeddings=100,\n",
    "    embedding_dim=64,\n",
    "    padding_idx = 0\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor([[1, 52, 73, 0]]) \n",
    "output = emb(input_ids)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(output.shape)\n",
    "print(output[0,3].sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2cd91280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5020751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[my] grad exists: True\n",
      "[nn] grad exists: True\n",
      "[my] grad shape: torch.Size([100, 64])\n",
      "[nn] grad shape: torch.Size([100, 64])\n",
      "[my] nonzero grad rows: [0, 1, 52, 73]\n",
      "[nn] nonzero grad rows: [0, 1, 52, 73]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "my_emb = Embedding(num_embeddings=100, embedding_dim=64, padding_idx=0).to(device)\n",
    "nn_emb = nn.Embedding(num_embeddings=100, embedding_dim=64, padding_idx=0).to(device)\n",
    "\n",
    "ids = torch.tensor([[1, 52, 73, 0]], dtype=torch.long, device=device)\n",
    "\n",
    "y_my = my_emb(ids)\n",
    "y_nn = nn_emb(ids)\n",
    "\n",
    "loss_my = y_my.sum()\n",
    "loss_nn = y_nn.sum()\n",
    "\n",
    "my_emb.zero_grad()\n",
    "nn_emb.weight.grad = None \n",
    "\n",
    "loss_my.backward()\n",
    "loss_nn.backward()\n",
    "\n",
    "print(\"[my] grad exists:\", my_emb.weight.grad is not None)\n",
    "print(\"[nn] grad exists:\", nn_emb.weight.grad is not None)\n",
    "\n",
    "print(\"[my] grad shape:\", my_emb.weight.grad.shape)\n",
    "print(\"[nn] grad shape:\", nn_emb.weight.grad.shape)\n",
    "\n",
    "my_nonzero_rows = (my_emb.weight.grad.abs().sum(dim=1) != 0).nonzero().flatten()\n",
    "nn_nonzero_rows = (nn_emb.weight.grad.abs().sum(dim=1) != 0).nonzero().flatten()\n",
    "\n",
    "print(\"[my] nonzero grad rows:\", my_nonzero_rows[:20].tolist())\n",
    "print(\"[nn] nonzero grad rows:\", nn_nonzero_rows[:20].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "781c128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[my] pad changed: True\n",
      "[nn] pad changed: True\n",
      "[my] pad row sum after: -6.400000095367432\n",
      "[nn] pad row sum after: -6.400000095367432\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "\n",
    "my_pad_before = my_emb.weight[0].detach().clone()\n",
    "nn_pad_before = nn_emb.weight[0].detach().clone()\n",
    "\n",
    "with torch.no_grad():\n",
    "    my_emb.weight -= lr * my_emb.weight.grad\n",
    "    nn_emb.weight -= lr * nn_emb.weight.grad\n",
    "\n",
    "my_pad_after = my_emb.weight[0].detach().clone()\n",
    "nn_pad_after = nn_emb.weight[0].detach().clone()\n",
    "\n",
    "print(\"[my] pad changed:\", not torch.allclose(my_pad_before, my_pad_after))\n",
    "print(\"[nn] pad changed:\", not torch.allclose(nn_pad_before, nn_pad_after))\n",
    "\n",
    "print(\"[my] pad row sum after:\", my_emb.weight[0].sum().item())\n",
    "print(\"[nn] pad row sum after:\", nn_emb.weight[0].sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3815ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66be83d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
